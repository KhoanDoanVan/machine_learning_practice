{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494af96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "QUY TR√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU - PART 3\n",
    "==================================\n",
    "Relationship Analysis & Pre-Modeling Preparation\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, pearsonr, spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TI·∫æP T·ª§C CLASS - RELATIONSHIP ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedAnalysisPipeline(DataCleaningPipeline):\n",
    "    \"\"\"\n",
    "    M·ªü r·ªông v·ªõi ph√¢n t√≠ch m·ªëi quan h·ªá v√† feature selection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=None, df=None, target_column=None):\n",
    "        super().__init__(data_path, df)\n",
    "        self.target_column = target_column\n",
    "        self.correlation_matrix = None\n",
    "        self.feature_importance = {}\n",
    "        self.pca_results = {}\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. PH√ÇN T√çCH T∆Ø∆†NG QUAN (CORRELATION ANALYSIS)\n",
    "    # ========================================================================\n",
    "    \n",
    "    def analyze_correlations(self, method='pearson', threshold=0.8, visualize=True):\n",
    "        \"\"\"\n",
    "        Ph√¢n t√≠ch t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            'pearson': T∆∞∆°ng quan tuy·∫øn t√≠nh\n",
    "            'spearman': T∆∞∆°ng quan th·ª© h·∫°ng\n",
    "            'kendall': T∆∞∆°ng quan Kendall Tau\n",
    "        threshold : float\n",
    "            Ng∆∞·ª°ng ƒë·ªÉ c·∫£nh b√°o multicollinearity\n",
    "        visualize : bool\n",
    "            V·∫Ω heatmap hay kh√¥ng\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîó PH√ÇN T√çCH T∆Ø∆†NG QUAN\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "        \n",
    "        # Ch·ªâ l·∫•y c√°c c·ªôt s·ªë\n",
    "        numeric_data = self.df_cleaned.select_dtypes(include=[np.number])\n",
    "        \n",
    "        if len(numeric_data.columns) < 2:\n",
    "            print(\"‚ö†Ô∏è  Kh√¥ng ƒë·ªß c·ªôt s·ªë ƒë·ªÉ ph√¢n t√≠ch t∆∞∆°ng quan\")\n",
    "            return None\n",
    "        \n",
    "        # T√≠nh correlation matrix\n",
    "        print(f\"\\nüìä Method: {method.upper()}\")\n",
    "        self.correlation_matrix = numeric_data.corr(method=method)\n",
    "        \n",
    "        # T√¨m c√°c c·∫∑p c√≥ t∆∞∆°ng quan cao\n",
    "        print(f\"\\nüî¥ C·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan cao (|r| > {threshold}):\")\n",
    "        high_corr_pairs = []\n",
    "        \n",
    "        for i in range(len(self.correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(self.correlation_matrix.columns)):\n",
    "                corr_val = self.correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > threshold:\n",
    "                    col1 = self.correlation_matrix.columns[i]\n",
    "                    col2 = self.correlation_matrix.columns[j]\n",
    "                    high_corr_pairs.append((col1, col2, corr_val))\n",
    "                    print(f\"  ‚Ä¢ {col1} ‚Üî {col2}: {corr_val:.3f}\")\n",
    "        \n",
    "        if not high_corr_pairs:\n",
    "            print(\"  ‚úì Kh√¥ng ph√°t hi·ªán multicollinearity nghi√™m tr·ªçng\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  T√¨m th·∫•y {len(high_corr_pairs)} c·∫∑p c√≥ multicollinearity!\")\n",
    "            print(\"  ‚Üí C√¢n nh·∫Øc lo·∫°i b·ªè m·ªôt trong c√°c bi·∫øn n√†y\")\n",
    "        \n",
    "        # Visualization\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "            \n",
    "            # Heatmap ƒë·∫ßy ƒë·ªß\n",
    "            sns.heatmap(\n",
    "                self.correlation_matrix, \n",
    "                annot=False, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                vmin=-1, vmax=1,\n",
    "                square=True,\n",
    "                ax=axes[0],\n",
    "                cbar_kws={'label': 'Correlation'}\n",
    "            )\n",
    "            axes[0].set_title(f'Ma tr·∫≠n T∆∞∆°ng quan ({method.capitalize()})', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # Top correlations v·ªõi target (n·∫øu c√≥)\n",
    "            if self.target_column and self.target_column in self.correlation_matrix.columns:\n",
    "                target_corr = self.correlation_matrix[self.target_column].drop(self.target_column).sort_values(ascending=False)\n",
    "                \n",
    "                # V·∫Ω top 20\n",
    "                top_n = min(20, len(target_corr))\n",
    "                target_corr_plot = pd.concat([\n",
    "                    target_corr.head(top_n//2),\n",
    "                    target_corr.tail(top_n//2)\n",
    "                ])\n",
    "                \n",
    "                colors = ['green' if x > 0 else 'red' for x in target_corr_plot.values]\n",
    "                axes[1].barh(range(len(target_corr_plot)), target_corr_plot.values, color=colors, alpha=0.7)\n",
    "                axes[1].set_yticks(range(len(target_corr_plot)))\n",
    "                axes[1].set_yticklabels(target_corr_plot.index, fontsize=9)\n",
    "                axes[1].set_xlabel('Correlation with Target')\n",
    "                axes[1].set_title(f'Top Features t∆∞∆°ng quan v·ªõi {self.target_column}', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "                axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "                axes[1].grid(True, alpha=0.3, axis='x')\n",
    "            else:\n",
    "                # V·∫Ω distribution of correlations\n",
    "                corr_values = self.correlation_matrix.values[np.triu_indices_from(self.correlation_matrix.values, k=1)]\n",
    "                axes[1].hist(corr_values, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "                axes[1].set_xlabel('Correlation Coefficient')\n",
    "                axes[1].set_ylabel('Frequency')\n",
    "                axes[1].set_title('Ph√¢n ph·ªëi c√°c gi√° tr·ªã T∆∞∆°ng quan', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "                axes[1].axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        self.report['correlation'] = {\n",
    "            'method': method,\n",
    "            'high_corr_pairs': high_corr_pairs\n",
    "        }\n",
    "        \n",
    "        return self.correlation_matrix\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. PH√ÇN T√çCH M·ªêI QUAN H·ªÜ FEATURE-TARGET\n",
    "    # ========================================================================\n",
    "    \n",
    "    def analyze_feature_target_relationship(self, target=None, task='auto'):\n",
    "        \"\"\"\n",
    "        Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa features v√† target\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target : str\n",
    "            T√™n c·ªôt target\n",
    "        task : str\n",
    "            'classification', 'regression', ho·∫∑c 'auto'\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ PH√ÇN T√çCH M·ªêI QUAN H·ªÜ FEATURE-TARGET\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if target is None:\n",
    "            target = self.target_column\n",
    "        \n",
    "        if target is None or target not in self.df_cleaned.columns:\n",
    "            print(\"‚ö†Ô∏è  C·∫ßn ch·ªâ ƒë·ªãnh target column h·ª£p l·ªá\")\n",
    "            return\n",
    "        \n",
    "        # Auto detect task type\n",
    "        if task == 'auto':\n",
    "            n_unique = self.df_cleaned[target].nunique()\n",
    "            task = 'classification' if n_unique < 20 else 'regression'\n",
    "        \n",
    "        print(f\"\\nüìã Target: {target}\")\n",
    "        print(f\"üìã Task: {task}\")\n",
    "        \n",
    "        # L·∫•y features\n",
    "        features = [col for col in self.df_cleaned.columns if col != target]\n",
    "        numeric_features = [col for col in features \n",
    "                           if pd.api.types.is_numeric_dtype(self.df_cleaned[col])]\n",
    "        \n",
    "        # 1. Statistical Tests\n",
    "        print(f\"\\nüî¨ STATISTICAL TESTS:\")\n",
    "        \n",
    "        if task == 'classification':\n",
    "            # ANOVA for numeric features\n",
    "            print(\"\\n  üìä ANOVA F-test (Numeric features):\")\n",
    "            anova_results = []\n",
    "            \n",
    "            for col in numeric_features[:20]:  # Top 20\n",
    "                groups = [self.df_cleaned[self.df_cleaned[target] == cat][col].dropna() \n",
    "                         for cat in self.df_cleaned[target].unique()]\n",
    "                \n",
    "                try:\n",
    "                    f_stat, p_value = f_oneway(*groups)\n",
    "                    anova_results.append({\n",
    "                        'Feature': col,\n",
    "                        'F-statistic': f_stat,\n",
    "                        'p-value': p_value,\n",
    "                        'Significant': '‚úì' if p_value < 0.05 else '‚úó'\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            anova_df = pd.DataFrame(anova_results).sort_values('F-statistic', ascending=False)\n",
    "            print(anova_df.head(10).to_string(index=False))\n",
    "        \n",
    "        else:  # regression\n",
    "            # Correlation with target\n",
    "            print(\"\\n  üìä Correlation with Target:\")\n",
    "            correlations = []\n",
    "            \n",
    "            for col in numeric_features[:20]:\n",
    "                try:\n",
    "                    corr, p_value = pearsonr(\n",
    "                        self.df_cleaned[col].dropna(), \n",
    "                        self.df_cleaned[target].loc[self.df_cleaned[col].dropna().index]\n",
    "                    )\n",
    "                    correlations.append({\n",
    "                        'Feature': col,\n",
    "                        'Correlation': corr,\n",
    "                        'p-value': p_value,\n",
    "                        'Significant': '‚úì' if p_value < 0.05 else '‚úó'\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            corr_df = pd.DataFrame(correlations).sort_values('Correlation', \n",
    "                                                            key=abs, ascending=False)\n",
    "            print(corr_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 2. Mutual Information\n",
    "        print(f\"\\nüîç MUTUAL INFORMATION:\")\n",
    "        X = self.df_cleaned[numeric_features].fillna(0)\n",
    "        y = self.df_cleaned[target]\n",
    "        \n",
    "        if task == 'classification':\n",
    "            mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        else:\n",
    "            mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "        \n",
    "        mi_df = pd.DataFrame({\n",
    "            'Feature': numeric_features,\n",
    "            'MI_Score': mi_scores\n",
    "        }).sort_values('MI_Score', ascending=False)\n",
    "        \n",
    "        print(mi_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 3. Visualization - Feature vs Target\n",
    "        print(f\"\\nüìä Visualizing top features...\")\n",
    "        \n",
    "        top_features = mi_df.head(6)['Feature'].tolist()\n",
    "        n_features = len(top_features)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(top_features):\n",
    "            if task == 'classification':\n",
    "                # Box plot for classification\n",
    "                self.df_cleaned.boxplot(column=feature, by=target, ax=axes[idx])\n",
    "                axes[idx].set_title(f'{feature} vs {target}')\n",
    "                axes[idx].set_xlabel('')\n",
    "            else:\n",
    "                # Scatter plot for regression\n",
    "                axes[idx].scatter(self.df_cleaned[feature], \n",
    "                                self.df_cleaned[target], \n",
    "                                alpha=0.5, s=20)\n",
    "                axes[idx].set_xlabel(feature)\n",
    "                axes[idx].set_ylabel(target)\n",
    "                axes[idx].set_title(f'{feature} vs {target}')\n",
    "                \n",
    "                # Add trend line\n",
    "                z = np.polyfit(self.df_cleaned[feature].fillna(0), \n",
    "                             self.df_cleaned[target], 1)\n",
    "                p = np.poly1d(z)\n",
    "                axes[idx].plot(self.df_cleaned[feature], \n",
    "                             p(self.df_cleaned[feature]), \n",
    "                             \"r--\", alpha=0.8, linewidth=2)\n",
    "            \n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self.feature_importance['mutual_information'] = mi_df\n",
    "        return mi_df\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. DIMENSIONALITY REDUCTION - PCA\n",
    "    # ========================================================================\n",
    "    \n",
    "    def perform_pca(self, n_components=None, variance_threshold=0.95, visualize=True):\n",
    "        \"\"\"\n",
    "        Th·ª±c hi·ªán Principal Component Analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int or None\n",
    "            S·ªë components mu·ªën gi·ªØ l·∫°i\n",
    "        variance_threshold : float\n",
    "            T·ª∑ l·ªá variance mu·ªën gi·ªØ l·∫°i\n",
    "        visualize : bool\n",
    "            C√≥ v·∫Ω bi·ªÉu ƒë·ªì hay kh√¥ng\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üî¨ PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "        \n",
    "        # Ch·ªâ l·∫•y numeric columns\n",
    "        numeric_data = self.df_cleaned.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Remove columns with NaN\n",
    "        numeric_data = numeric_data.dropna(axis=1)\n",
    "        \n",
    "        if len(numeric_data.columns) < 2:\n",
    "            print(\"‚ö†Ô∏è  Kh√¥ng ƒë·ªß features s·ªë ƒë·ªÉ th·ª±c hi·ªán PCA\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nüìä S·ªë features: {len(numeric_data.columns)}\")\n",
    "        \n",
    "        # Standardize data\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(numeric_data)\n",
    "        \n",
    "        # Fit PCA v·ªõi t·∫•t c·∫£ components\n",
    "        pca_full = PCA()\n",
    "        pca_full.fit(data_scaled)\n",
    "        \n",
    "        # Explained variance\n",
    "        cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "        \n",
    "        # Determine optimal n_components\n",
    "        if n_components is None:\n",
    "            n_components = np.argmax(cumsum_variance >= variance_threshold) + 1\n",
    "        \n",
    "        print(f\"\\nüéØ S·ªë components ƒë∆∞·ª£c ch·ªçn: {n_components}\")\n",
    "        print(f\"   (Gi·∫£i th√≠ch {cumsum_variance[n_components-1]*100:.2f}% variance)\")\n",
    "        \n",
    "        # Fit PCA v·ªõi n_components\n",
    "        pca = PCA(n_components=n_components)\n",
    "        components = pca.fit_transform(data_scaled)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        pca_df = pd.DataFrame(\n",
    "            components,\n",
    "            columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "        )\n",
    "        \n",
    "        # Component loadings\n",
    "        loadings = pd.DataFrame(\n",
    "            pca.components_.T,\n",
    "            columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "            index=numeric_data.columns\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìã Top 5 features cho m·ªói PC:\")\n",
    "        for i in range(min(3, n_components)):\n",
    "            pc_name = f'PC{i+1}'\n",
    "            top_features = loadings[pc_name].abs().sort_values(ascending=False).head(5)\n",
    "            print(f\"\\n  {pc_name} (Variance: {pca.explained_variance_ratio_[i]*100:.2f}%):\")\n",
    "            for feat, loading in top_features.items():\n",
    "                print(f\"    ‚Ä¢ {feat}: {loading:.3f}\")\n",
    "        \n",
    "        # Visualization\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 1. Scree plot\n",
    "            axes[0, 0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n",
    "                          pca_full.explained_variance_ratio_,\n",
    "                          alpha=0.7, color='steelblue')\n",
    "            axes[0, 0].set_xlabel('Principal Component')\n",
    "            axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "            axes[0, 0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. Cumulative variance\n",
    "            axes[0, 1].plot(range(1, len(cumsum_variance)+1), \n",
    "                          cumsum_variance, \n",
    "                          marker='o', linewidth=2, color='green')\n",
    "            axes[0, 1].axhline(y=variance_threshold, color='r', \n",
    "                             linestyle='--', label=f'{variance_threshold*100}% threshold')\n",
    "            axes[0, 1].axvline(x=n_components, color='orange', \n",
    "                             linestyle='--', label=f'{n_components} components')\n",
    "            axes[0, 1].set_xlabel('Number of Components')\n",
    "            axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "            axes[0, 1].set_title('Cumulative Variance Explained', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Biplot (PC1 vs PC2)\n",
    "            if n_components >= 2:\n",
    "                axes[1, 0].scatter(components[:, 0], components[:, 1], \n",
    "                                 alpha=0.5, s=30)\n",
    "                axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "                axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "                axes[1, 0].set_title('PCA Biplot (PC1 vs PC2)', \n",
    "                                   fontsize=12, fontweight='bold')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add loading vectors (top 10 features)\n",
    "                top_loadings = loadings[['PC1', 'PC2']].abs().sum(axis=1).sort_values(ascending=False).head(10)\n",
    "                scale = 3\n",
    "                for feature in top_loadings.index:\n",
    "                    axes[1, 0].arrow(0, 0, \n",
    "                                   loadings.loc[feature, 'PC1']*scale,\n",
    "                                   loadings.loc[feature, 'PC2']*scale,\n",
    "                                   head_width=0.1, head_length=0.1,\n",
    "                                   fc='red', ec='red', alpha=0.6)\n",
    "                    axes[1, 0].text(loadings.loc[feature, 'PC1']*scale*1.1,\n",
    "                                  loadings.loc[feature, 'PC2']*scale*1.1,\n",
    "                                  feature, fontsize=8, ha='center')\n",
    "            \n",
    "            # 4. Heatmap of loadings\n",
    "            top_n = min(15, len(loadings))\n",
    "            top_features_idx = loadings.abs().sum(axis=1).sort_values(ascending=False).head(top_n).index\n",
    "            \n",
    "            sns.heatmap(loadings.loc[top_features_idx].T, \n",
    "                       cmap='coolwarm', center=0,\n",
    "                       annot=True, fmt='.2f',\n",
    "                       ax=axes[1, 1],\n",
    "                       cbar_kws={'label': 'Loading'})\n",
    "            axes[1, 1].set_title(f'Component Loadings (Top {top_n} Features)', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Save results\n",
    "        self.pca_results = {\n",
    "            'pca_model': pca,\n",
    "            'components': pca_df,\n",
    "            'loadings': loadings,\n",
    "            'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "            'n_components': n_components\n",
    "        }\n",
    "        \n",
    "        return pca_df, loadings\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. FEATURE SELECTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def select_features(self, target=None, method='all', k=20):\n",
    "        \"\"\"\n",
    "        Feature Selection s·ª≠ d·ª•ng nhi·ªÅu ph∆∞∆°ng ph√°p\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target : str\n",
    "            T√™n c·ªôt target\n",
    "        method : str\n",
    "            'univariate': SelectKBest\n",
    "            'rfe': Recursive Feature Elimination\n",
    "            'importance': Tree-based feature importance\n",
    "            'all': K·∫øt h·ª£p t·∫•t c·∫£\n",
    "        k : int\n",
    "            S·ªë features mu·ªën ch·ªçn\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéØ FEATURE SELECTION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if target is None:\n",
    "            target = self.target_column\n",
    "        \n",
    "        if target is None or target not in self.df_cleaned.columns:\n",
    "            print(\"‚ö†Ô∏è  C·∫ßn ch·ªâ ƒë·ªãnh target column h·ª£p l·ªá\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare data\n",
    "        X = self.df_cleaned.drop(columns=[target]).select_dtypes(include=[np.number])\n",
    "        y = self.df_cleaned[target]\n",
    "        \n",
    "        # Remove NaN\n",
    "        valid_idx = X.notna().all(axis=1) & y.notna()\n",
    "        X = X[valid_idx].fillna(0)\n",
    "        y = y[valid_idx]\n",
    "        \n",
    "        # Determine task type\n",
    "        task = 'classification' if y.nunique() < 20 else 'regression'\n",
    "        print(f\"\\nüìã Task: {task}\")\n",
    "        print(f\"üìã Features: {len(X.columns)}\")\n",
    "        print(f\"üìã Target: {target}\")\n",
    "        \n",
    "        selected_features = {}\n",
    "        \n",
    "        # 1. Univariate Selection\n",
    "        if method in ['univariate', 'all']:\n",
    "            print(f\"\\nüîπ Method 1: Univariate Selection (SelectKBest)\")\n",
    "            \n",
    "            if task == 'classification':\n",
    "                selector = SelectKBest(score_func=f_classif, k=min(k, len(X.columns)))\n",
    "            else:\n",
    "                selector = SelectKBest(score_func=f_regression, k=min(k, len(X.columns)))\n",
    "            \n",
    "            selector.fit(X, y)\n",
    "            \n",
    "            scores_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Score': selector.scores_\n",
    "            }).sort_values('Score', ascending=False)\n",
    "            \n",
    "            selected_features['univariate'] = scores_df.head(k)['Feature'].tolist()\n",
    "            print(f\"   Top 10 features:\")\n",
    "            print(scores_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 2. RFE (Recursive Feature Elimination)\n",
    "        if method in ['rfe', 'all']:\n",
    "            print(f\"\\nüîπ Method 2: Recursive Feature Elimination (RFE)\")\n",
    "            \n",
    "            if task == 'classification':\n",
    "                estimator = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "            else:\n",
    "                estimator = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "            \n",
    "            rfe = RFE(estimator=estimator, n_features_to_select=min(k, len(X.columns)))\n",
    "            rfe.fit(X, y)\n",
    "            \n",
    "            rfe_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Ranking': rfe.ranking_,\n",
    "                'Selected': rfe.support_\n",
    "            }).sort_values('Ranking')\n",
    "            \n",
    "            selected_features['rfe'] = rfe_df[rfe_df['Selected']]['Feature'].tolist()\n",
    "            print(f\"   Selected {len(selected_features['rfe'])} features\")\n",
    "            print(rfe_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 3. Feature Importance\n",
    "        if method in ['importance', 'all']:\n",
    "            print(f\"\\nüîπ Method 3: Tree-based Feature Importance\")\n",
    "            \n",
    "            if task == 'classification':\n",
    "                model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            else:\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            \n",
    "            model.fit(X, y)\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            selected_features['importance'] = importance_df.head(k)['Feature'].tolist()\n",
    "            print(f\"   Top 10 features:\")\n",
    "            print(importance_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 4. Combine results\n",
    "        if method == 'all':\n",
    "            print(f\"\\nüéØ ENSEMBLE FEATURE SELECTION:\")\n",
    "            \n",
    "            # Count votes\n",
    "            from collections import Counter\n",
    "            all_features = []\n",
    "            for features_list in selected_features.values():\n",
    "                all_features.extend(features_list)\n",
    "            \n",
    "            feature_votes = Counter(all_features)\n",
    "            \n",
    "            ensemble_df = pd.DataFrame(\n",
    "                feature_votes.items(), \n",
    "                columns=['Feature', 'Votes']\n",
    "            ).sort_values('Votes', ascending=False)\n",
    "            \n",
    "            print(f\"\\n   Features xu·∫•t hi·ªán trong nhi·ªÅu methods nh·∫•t:\")\n",
    "            print(ensemble_df.head(15).to_string(index=False))\n",
    "            \n",
    "            # Select features with at least 2 votes\n",
    "            final_features = ensemble_df[ensemble_df['Votes'] >= 2]['Feature'].tolist()\n",
    "            selected_features['ensemble'] = final_features[:k]\n",
    "            \n",
    "            print(f\"\\n‚úÖ Ch·ªçn {len(final_features[:k])} features cu·ªëi c√πng (‚â•2 votes)\")\n",
    "        \n",
    "        # Visualization\n",
    "        if method == 'all':\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            \n",
    "            ensemble_df_plot = ensemble_df.head(20)\n",
    "            colors = ['green' if x >= 2 else 'orange' for x in ensemble_df_plot['Votes']]\n",
    "            \n",
    "            ax.barh(range(len(ensemble_df_plot)), \n",
    "                   ensemble_df_plot['Votes'].values,\n",
    "                   color=colors, alpha=0.7)\n",
    "            ax.set_yticks(range(len(ensemble_df_plot)))\n",
    "            ax.set_yticklabels(ensemble_df_plot['Feature'].values)\n",
    "            ax.set_xlabel('Number of Votes (Methods)', fontsize=11)\n",
    "            ax.set_title('Feature Selection - Ensemble Results', \n",
    "                        fontsize=13, fontweight='bold')\n",
    "            ax.axvline(x=2, color='red', linestyle='--', \n",
    "                      linewidth=2, label='Threshold (2 votes)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        self.feature_importance['selected_features'] = selected_features\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. T·∫†O FINAL DATASET CHO MODELING\n",
    "    # ========================================================================\n",
    "    \n",
    "    def prepare_final_dataset(self, selected_features=None, include_pca=False):\n",
    "        \"\"\"\n",
    "        T·∫°o dataset cu·ªëi c√πng s·∫µn s√†ng cho modeling\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        selected_features : list\n",
    "            Danh s√°ch features ƒë∆∞·ª£c ch·ªçn\n",
    "        include_pca : bool\n",
    "            C√≥ th√™m PCA components kh√¥ng\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéÅ CHU·∫®N B·ªä FINAL DATASET\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_final = self.df_cleaned.copy()\n",
    "        \n",
    "        # 1. Select features\n",
    "        if selected_features is not None:\n",
    "            if self.target_column:\n",
    "                cols_to_keep = selected_features + [self.target_column]\n",
    "            else:\n",
    "                cols_to_keep = selected_features\n",
    "            \n",
    "            cols_to_keep = [c for c in cols_to_keep if c in df_final.columns]\n",
    "            df_final = df_final[cols_to_keep]\n",
    "            print(f\"\\n‚úì Ch·ªçn {len(selected_features)} features\")\n",
    "        \n",
    "        # 2. Add PCA components\n",
    "        if include_pca and self.pca_results:\n",
    "            pca_df = self.pca_results['components']\n",
    "            df_final = pd.concat([df_final, pca_df], axis=1)\n",
    "            print(f\"‚úì Th√™m {len(pca_df.columns)} PCA components\")\n",
    "        \n",
    "        # 3. Final checks\n",
    "        print(f\"\\nüìä FINAL DATASET:\")\n",
    "        print(f\"  ‚Ä¢ Shape: {df_final.shape}\")\n",
    "        print(f\"  ‚Ä¢ Missing values: {df_final.isnull().sum().sum()}\")\n",
    "        print(f\"  ‚Ä¢ Memory: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # 4. Summary report\n",
    "        print(f\"\\nüìã FEATURES SUMMARY:\")\n",
    "        numeric_cols = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if self.target_column in numeric_cols:\n",
    "            numeric_cols.remove(self.target_column)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Numeric features: {len(numeric_cols)}\")\n",
    "        print(f\"  ‚Ä¢ Target: {self.target_column}\")\n",
    "        \n",
    "        # Save\n",
    "        self.df_final = df_final\n",
    "        \n",
    "        print(f\"\\nüíæ ƒê·ªÉ truy c·∫≠p:\")\n",
    "        print(f\"   df_final = pipeline.df_final\")\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. B√ÅO C√ÅO T·ªîNG K·∫æT HO√ÄN CH·ªàNH\n",
    "    # ========================================================================\n",
    "    \n",
    "    def generate_complete_report(self, save_path=None):\n",
    "        \"\"\"\n",
    "        T·∫°o b√°o c√°o t·ªïng k·∫øt to√†n b·ªô pipeline\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä B√ÅO C√ÅO T·ªîNG K·∫æT PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        report = {\n",
    "            'original_shape': self.df.shape,\n",
    "            'final_shape': self.df_final.shape if hasattr(self, 'df_final') else self.df_cleaned.shape,\n",
    "            'processing_steps': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüî¢ THAY ƒê·ªîI D·ªÆ LI·ªÜU:\")\n",
    "        print(f\"  ‚Ä¢ Shape ban ƒë·∫ßu: {self.df.shape}\")\n",
    "        if hasattr(self, 'df_final'):\n",
    "            print(f\"  ‚Ä¢ Shape cu·ªëi c√πng: {self.df_final.shape}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Shape sau cleaning: {self.df_cleaned.shape}\")\n",
    "        \n",
    "        print(f\"\\nüîß C√ÅC B∆Ø·ªöC ƒê√É TH·ª∞C HI·ªÜN:\")\n",
    "        \n",
    "        # Missing values\n",
    "        if self.imputation_strategies:\n",
    "            print(f\"  ‚úì Missing Value Imputation: {len(self.imputation_strategies)} c·ªôt\")\n",
    "            report['processing_steps'].append('imputation')\n",
    "        \n",
    "        # Encoding\n",
    "        if self.encoding_strategies:\n",
    "            print(f\"  ‚úì Categorical Encoding: {len(self.encoding_strategies)} c·ªôt\")\n",
    "            report['processing_steps'].append('encoding')\n",
    "        \n",
    "        # Scaling\n",
    "        if self.scaling_strategies:\n",
    "            print(f\"  ‚úì Feature Scaling: {self.scaling_strategies.get('method', 'N/A')}\")\n",
    "            report['processing_steps'].append('scaling')\n",
    "        \n",
    "        # Outliers\n",
    "        if 'outliers' in self.report:\n",
    "            print(f\"  ‚úì Outlier Detection & Handling\")\n",
    "            report['processing_steps'].append('outlier_handling')\n",
    "        \n",
    "        # PCA\n",
    "        if self.pca_results:\n",
    "            n_comp = self.pca_results.get('n_components', 0)\n",
    "            variance = sum(self.pca_results.get('explained_variance_ratio', []))\n",
    "            print(f\"  ‚úì PCA: {n_comp} components ({variance*100:.1f}% variance)\")\n",
    "            report['processing_steps'].append('pca')\n",
    "        \n",
    "        # Feature Selection\n",
    "        if self.feature_importance:\n",
    "            print(f\"  ‚úì Feature Selection\")\n",
    "            report['processing_steps'].append('feature_selection')\n",
    "        \n",
    "        print(f\"\\n‚úÖ PIPELINE HO√ÄN TH√ÄNH!\")\n",
    "        print(f\"   D·ªØ li·ªáu s·∫µn s√†ng cho modeling\")\n",
    "        \n",
    "        # Save report\n",
    "        if save_path:\n",
    "            import json\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "            print(f\"\\nüíæ B√°o c√°o ƒë√£ l∆∞u t·∫°i: {save_path}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG HO√ÄN CH·ªàNH - FULL PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "====================================\n",
    "H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG COMPLETE PIPELINE\n",
    "====================================\n",
    "\n",
    "# ============ B∆Ø·ªöC 1: KH·ªûI T·∫†O ============\n",
    "pipeline = AdvancedAnalysisPipeline(\n",
    "    df=your_dataframe,\n",
    "    target_column='your_target'  # Optional\n",
    ")\n",
    "\n",
    "# ============ B∆Ø·ªöC 2: EDA ============\n",
    "# Part 1: Ph√¢n t√≠ch ban ƒë·∫ßu\n",
    "pipeline.initial_assessment()\n",
    "pipeline.descriptive_statistics()\n",
    "pipeline.analyze_missing_values(plot=True)\n",
    "pipeline.visualize_distributions(max_cols=12)\n",
    "\n",
    "# ============ B∆Ø·ªöC 3: CLEANING ============\n",
    "# Part 2: L√†m s·∫°ch v√† preprocessing\n",
    "pipeline.handle_missing_values(threshold=0.7)\n",
    "pipeline.detect_outliers(methods=['iqr', 'zscore'], visualize=True)\n",
    "pipeline.handle_outliers(strategy='cap')\n",
    "pipeline.encode_categorical(max_categories=10)\n",
    "pipeline.scale_features(method='standard')\n",
    "pipeline.clean_data_inconsistencies()\n",
    "\n",
    "# ============ B∆Ø·ªöC 4: ADVANCED ANALYSIS ============\n",
    "# Part 3: Ph√¢n t√≠ch n√¢ng cao\n",
    "pipeline.analyze_correlations(method='pearson', threshold=0.8, visualize=True)\n",
    "pipeline.analyze_feature_target_relationship(task='auto')\n",
    "pca_components, loadings = pipeline.perform_pca(variance_threshold=0.95, visualize=True)\n",
    "selected_features = pipeline.select_features(method='all', k=20)\n",
    "\n",
    "# ============ B∆Ø·ªöC 5: FINAL DATASET ============\n",
    "df_final = pipeline.prepare_final_dataset(\n",
    "    selected_features=selected_features.get('ensemble', None),\n",
    "    include_pca=False\n",
    ")\n",
    "\n",
    "# ============ B∆Ø·ªöC 6: B√ÅO C√ÅO & L∆ØU ============\n",
    "report = pipeline.generate_complete_report(save_path='pipeline_report.json')\n",
    "\n",
    "# L∆∞u d·ªØ li·ªáu cu·ªëi c√πng\n",
    "df_final.to_csv('cleaned_data_final.csv', index=False)\n",
    "df_final.to_parquet('cleaned_data_final.parquet', index=False)\n",
    "\n",
    "# ============ TRUY C·∫¨P K·∫æT QU·∫¢ ============\n",
    "# D·ªØ li·ªáu g·ªëc: pipeline.df\n",
    "# D·ªØ li·ªáu ƒë√£ cleaned: pipeline.df_cleaned\n",
    "# D·ªØ li·ªáu cu·ªëi c√πng: pipeline.df_final\n",
    "# Ma tr·∫≠n t∆∞∆°ng quan: pipeline.correlation_matrix\n",
    "# PCA results: pipeline.pca_results\n",
    "# Feature importance: pipeline.feature_importance\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ƒê√É HO√ÄN TH√ÄNH T·∫§T C·∫¢ 3 PARTS!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìö PIPELINE BAO G·ªíM:\")\n",
    "print(\"  Part 1: Data Loading & Initial EDA\")\n",
    "print(\"  Part 2: Data Cleaning & Preprocessing\")\n",
    "print(\"  Part 3: Relationship Analysis & Feature Selection\")\n",
    "print(\"\\nüéâ B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng pipeline n√†y cho B·∫§T K·ª≤ t·∫≠p d·ªØ li·ªáu n√†o!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

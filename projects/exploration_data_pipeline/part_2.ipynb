{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "QUY TR√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU - PART 2\n",
    "==================================\n",
    "Data Cleaning & Advanced Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from part_2 import DataExplorationPipeline\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TI·∫æP T·ª§C CLASS DataExplorationPipeline\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DataCleaningPipeline(DataExplorationPipeline):\n",
    "    \"\"\"\n",
    "    M·ªü r·ªông class v·ªõi c√°c ch·ª©c nƒÉng cleaning v√† preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path=None, df=None):\n",
    "        super().__init__(data_path, df)\n",
    "        self.df_cleaned = None\n",
    "        self.imputation_strategies = {}\n",
    "        self.scaling_strategies = {}\n",
    "        self.encoding_strategies = {}\n",
    "\n",
    "    \n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. X·ª¨ L√ù GI√Å TR·ªä B·ªä THI·∫æU (MISSING VALUES HANDLING)\n",
    "    # ========================================================================\n",
    "\n",
    "    def handle_missing_values(self, strategies=None, threshold=0.7):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω gi√° tr·ªã b·ªã thi·∫øu v·ªõi nhi·ªÅu chi·∫øn l∆∞·ª£c\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        strategies : dict\n",
    "            Dictionary ch·ª©a chi·∫øn l∆∞·ª£c cho t·ª´ng c·ªôt\n",
    "            V√≠ d·ª•: {'column_name': 'mean', 'other_col': 'knn'}\n",
    "        threshold : float\n",
    "            Ng∆∞·ª°ng ƒë·ªÉ x√≥a c·ªôt (n·∫øu t·ª∑ l·ªá thi·∫øu > threshold)\n",
    "        \n",
    "        Strategies available:\n",
    "        - 'drop': X√≥a h√†ng\n",
    "        - 'mean': ƒêi·ªÅn trung b√¨nh\n",
    "        - 'median': ƒêi·ªÅn trung v·ªã\n",
    "        - 'mode': ƒêi·ªÅn ch·∫ø ƒë·ªô (mode)\n",
    "        - 'constant': ƒêi·ªÅn gi√° tr·ªã c·ªë ƒë·ªãnh\n",
    "        - 'ffill': Forward fill\n",
    "        - 'bfill': Backward fill\n",
    "        - 'knn': KNN Imputer\n",
    "        - 'iterative': MICE/Iterative Imputer\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîß X·ª¨ L√ù GI√Å TR·ªä B·ªä THI·∫æU\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "        self.df_cleaned = self.df.copy()\n",
    "\n",
    "\n",
    "        # B∆∞·ªõc 1: X√≥a c√°c c·ªôt c√≥ qu√° nhi·ªÅu missing\n",
    "        missing_pct = self.df_cleaned.isnull().sum() / len(self.df_cleaned)\n",
    "        cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "        if cols_to_drop:\n",
    "            print(f\"\\nüóëÔ∏è  X√≥a {len(cols_to_drop)} c·ªôt c√≥ >70% gi√° tr·ªã thi·∫øu:\")\n",
    "            for col in cols_to_drop:\n",
    "                print(f\"  ‚Ä¢ {col}: {missing_pct[col]*100:.1f}% thi·∫øu\")\n",
    "\n",
    "            self.df_cleaned.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        # B∆∞·ªõc 2: √Åp d·ª•ng strategies\n",
    "        if strategies is None:\n",
    "            strategies = self._auto_imputation_strategy()\n",
    "\n",
    "        print(\"\\nüìã Strategies ƒë∆∞·ª£c √°p d·ª•ng:\")\n",
    "\n",
    "        for col, strategy in strategies.items():\n",
    "            if col not in self.df_cleaned.columns:\n",
    "                continue\n",
    "\n",
    "            missing_count = self.df_cleaned[col].isnull().sum()\n",
    "            if missing_count == 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n  ‚Üí {col} ({missing_count} gi√° tr·ªã thi·∫øu):\")\n",
    "            print(f\"     Strategy: {strategy}\")\n",
    "\n",
    "            try:\n",
    "                if strategy == 'drop':\n",
    "                    self.df_cleaned.dropna(subset=[col], inplace=True)\n",
    "                \n",
    "                elif strategy == 'mean':\n",
    "                    self.df_cleaned[col].fillna(self.df_cleaned[col].mean(), inplace=True)\n",
    "                \n",
    "                elif strategy == 'median':\n",
    "                    self.df_cleaned[col].fillna(self.df_cleaned[col].median(), inplace=True)\n",
    "                \n",
    "                elif strategy == 'mode':\n",
    "                    mode_val = self.df_cleaned[col].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        self.df_cleaned[col].fillna(mode_val[0], inplace=True)\n",
    "                \n",
    "                elif strategy == 'constant':\n",
    "                    fill_val = 0 if pd.api.types.is_numeric_dtype(self.df_cleaned[col]) else 'Unknown'\n",
    "                    self.df_cleaned[col].fillna(fill_val, inplace=True)\n",
    "                \n",
    "                elif strategy == 'ffill':\n",
    "                    self.df_cleaned[col].fillna(method='ffill', inplace=True)\n",
    "                \n",
    "                elif strategy == 'bfill':\n",
    "                    self.df_cleaned[col].fillna(method='bfill', inplace=True)\n",
    "                \n",
    "                elif strategy == 'knn':\n",
    "                    self._apply_knn_imputer([col])\n",
    "                \n",
    "                elif strategy == 'iterative':\n",
    "                    self._apply_iterative_imputer([col])\n",
    "                \n",
    "                self.imputation_strategies[col] = strategy\n",
    "                print(f\"     ‚úì Ho√†n th√†nh\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚úó L·ªói: {str(e)}\")\n",
    "\n",
    "        # B√°o c√°o k·∫øt qu·∫£\n",
    "        remaining_missing = self.df_cleaned.isnull().sum().sum()\n",
    "        print(f\"\\n‚úÖ K·∫øt qu·∫£:\")\n",
    "        print(f\"  ‚Ä¢ Gi√° tr·ªã thi·∫øu c√≤n l·∫°i: {remaining_missing}\")\n",
    "        print(f\"  ‚Ä¢ S·ªë h√†ng: {len(self.df)} ‚Üí {len(self.df_cleaned)}\")\n",
    "        print(f\"  ‚Ä¢ S·ªë c·ªôt: {len(self.df.columns)} ‚Üí {len(self.df_cleaned.columns)}\")\n",
    "        \n",
    "\n",
    "        return self.df_cleaned\n",
    "    \n",
    "\n",
    "\n",
    "    def _auto_imputation_strategy(self):\n",
    "        \"\"\"T·ª± ƒë·ªông ch·ªçn strategy ph√π h·ª£p cho t·ª´ng c·ªôt\"\"\"\n",
    "\n",
    "        strategies = {}\n",
    "\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].isnull().sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Numeric columns\n",
    "            if col in self.numeric_cols:\n",
    "                # N·∫øu ph√¢n ph·ªëi g·∫ßn normal ‚Üí mean, ng∆∞·ª£c l·∫°i ‚Üí median\n",
    "                skewness = abs(self.df[col].skew())\n",
    "                strategies[col] = 'median' if skewness > 1 else 'mean'\n",
    "\n",
    "            # Categorical columns\n",
    "            elif col in self.categorical_cols:\n",
    "                strategies[col] = 'mode'\n",
    "\n",
    "            # Datetime columns\n",
    "            elif col in self.datetime_cols:\n",
    "                strategies[col] = 'ffill'\n",
    "\n",
    "        \n",
    "        return strategies\n",
    "    \n",
    "\n",
    "\n",
    "    def _apply_knn_imputer(self, columns, n_neighbors=5):\n",
    "        \"\"\"√Åp d·ª•ng KNN Imputer cho c√°c c·ªôt s·ªë\"\"\"\n",
    "\n",
    "        numeric_cols = [c for c in columns if c in self.numeric_cols]\n",
    "        if not numeric_cols:\n",
    "            return\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        self.df_cleaned[numeric_cols] = imputer.fit_transform(self.df_cleaned[numeric_cols])\n",
    "\n",
    "\n",
    "\n",
    "    def _apply_iterative_imputer(self, columns, max_iter=10):\n",
    "        \"\"\"√Åp d·ª•ng Iterative Imputer (MICE)\"\"\"\n",
    "\n",
    "        numeric_cols = [c for c in columns if c in self.numeric_cols]\n",
    "        if not numeric_cols:\n",
    "            return\n",
    "        \n",
    "        imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "        self.df_cleaned[numeric_cols] = imputer.fit_transform(self.df_cleaned[numeric_cols])\n",
    "\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. PH√ÅT HI·ªÜN V√Ä X·ª¨ L√ù NGO·∫†I LAI (OUTLIER DETECTION & HANDLING)\n",
    "    # ========================================================================\n",
    "    \n",
    "\n",
    "    def detect_outliers(self, methods=['iqr', 'zscore'], visualize=True):\n",
    "        \"\"\"\n",
    "        Ph√°t hi·ªán outliers b·∫±ng nhi·ªÅu ph∆∞∆°ng ph√°p\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        methods : list\n",
    "            C√°c ph∆∞∆°ng ph√°p: 'iqr', 'zscore', 'isolation_forest'\n",
    "        visualize : bool\n",
    "            C√≥ v·∫Ω box plot hay kh√¥ng\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîç PH√ÅT HI·ªÜN NGO·∫†I LAI (OUTLIERS)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "\n",
    "        outlier_info = {}\n",
    "\n",
    "        for col in self.numeric_cols:\n",
    "            if col not in self.df_cleaned.columns:\n",
    "                continue\n",
    "\n",
    "            outliers = {}\n",
    "            data = self.df_cleaned[col].dropna()\n",
    "\n",
    "            # Ph∆∞∆°ng ph√°p IQR\n",
    "            if 'iqr' in methods:\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "                outliers['iqr'] = {\n",
    "                    'count': iqr_outliers,\n",
    "                    'percentage': iqr_outliers / len(data) * 100,\n",
    "                    'lower_bound': lower_bound,\n",
    "                    'upper_bound': upper_bound\n",
    "                }\n",
    "\n",
    "            # Ph∆∞∆°ng ph√°p Z-Score\n",
    "            if 'zscore' in methods:\n",
    "                z_scores = np.abs(stats.zscore(data))\n",
    "                zscore_outliers = (z_scores > 3).sum()\n",
    "                outliers['zscore'] = {\n",
    "                    'count': zscore_outliers,\n",
    "                    'percentage': zscore_outliers / len(data) * 100\n",
    "                }\n",
    "            \n",
    "            outlier_info[col] = outliers\n",
    "\n",
    "\n",
    "        # In b√°o c√°o\n",
    "        print(\"\\nüìä B√°o c√°o Outliers:\")\n",
    "        for col, methods_result in outlier_info.items():\n",
    "            print(f\"\\n  {col}:\")\n",
    "            for method, result in methods_result.items():\n",
    "                print(f\"    ‚Ä¢ {method.upper()}: {result['count']} outliers ({result['percentage']:.2f}%)\")\n",
    "                if 'lower_bound' in result:\n",
    "                    print(f\"      Range: [{result['lower_bound']:.2f}, {result['upper_bound']:.2f}]\")\n",
    "        \n",
    "\n",
    "        # Visualization\n",
    "        if visualize and len(self.numeric_cols) > 0:if visualize and len(self.numeric_cols) > 0:\n",
    "            cols_to_plot = [c for c in self.numeric_cols[:12] if c in self.df_cleaned.columns]\n",
    "            n_cols = 3\n",
    "            n_rows = (len(cols_to_plot) + n_cols - 1) // n_cols\n",
    "        \n",
    "\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "            axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "        \n",
    "\n",
    "            for idx, col in enumerate(cols_to_plot):\n",
    "                self.df_cleaned.boxplot(column=col, ax=axes[idx])\n",
    "                axes[idx].set_title(f'{col}', fontsize=10)\n",
    "                axes[idx].set_ylabel('')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "            \n",
    "            for idx in range(len(cols_to_plot), len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "\n",
    "            plt.suptitle('Box Plots - Ph√°t hi·ªán Outliers', fontsize=14, fontweight='bold', y=1.00)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        self.report['outliers'] = outlier_info\n",
    "        return outlier_info\n",
    "    \n",
    "\n",
    "    def handle_outliers(self, strategy='cap', columns=None, iqr_multiplier=1.5):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω outliers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        strategy : str\n",
    "            'cap': Gi·ªõi h·∫°n (Winsorization)\n",
    "            'remove': Lo·∫°i b·ªè\n",
    "            'log': Bi·∫øn ƒë·ªïi log\n",
    "            'boxcox': Bi·∫øn ƒë·ªïi Box-Cox\n",
    "        columns : list\n",
    "            Danh s√°ch c·ªôt c·∫ßn x·ª≠ l√Ω (None = t·∫•t c·∫£ numeric)\n",
    "        iqr_multiplier : float\n",
    "            H·ªá s·ªë nh√¢n v·ªõi IQR\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîß X·ª¨ L√ù NGO·∫†I LAI\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "\n",
    "        if columns is None:\n",
    "            columns = self.numeric_cols\n",
    "\n",
    "        columns = [c for c in columns if c in self.df_cleaned.columns]\n",
    "\n",
    "        print(f\"\\nStrategy: {strategy}\")\n",
    "        print(f\"S·ªë c·ªôt x·ª≠ l√Ω: {len(columns)}\\n\")\n",
    "\n",
    "        for col in columns:\n",
    "            data = self.df_cleaned[col].copy()\n",
    "            original_count = len(data)\n",
    "\n",
    "            if strategy == 'cap':\n",
    "                # Winsorization\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                upper_bound = Q3 + iqr_multiplier * IQR\n",
    "\n",
    "                # Cap values\n",
    "                capped = self.df_cleaned[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                changed = (self.df_cleaned[col] != capped).sum()\n",
    "                self.df_cleaned[col] = capped\n",
    "\n",
    "                print(f\"  ‚Ä¢ {col}: {changed} gi√° tr·ªã b·ªã cap\")\n",
    "\n",
    "            elif strategy == 'remove':\n",
    "                # Remove outliers\n",
    "\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                \n",
    "                mask = (data >= lower_bound) & (data <= upper_bound)\n",
    "                self.df_cleaned = self.df_cleaned[mask]\n",
    "                removed = original_count - len(self.df_cleaned)\n",
    "                \n",
    "                print(f\"  ‚Ä¢ {col}: {removed} h√†ng b·ªã lo·∫°i b·ªè\")\n",
    "\n",
    "\n",
    "            elif strategy == 'log':\n",
    "                # Log transformation\n",
    "                if (data > 0).all():\n",
    "                    self.df_cleaned[col] = np.log1p(data)\n",
    "                    print(f\"  ‚Ä¢ {col}: ƒê√£ √°p d·ª•ng log transform\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {col}: B·ªè qua (c√≥ gi√° tr·ªã <= 0)\")\n",
    "\n",
    "\n",
    "            elif strategy == 'boxcox':\n",
    "                # Box-Cox transformation\n",
    "                if (data > 0).all():\n",
    "                    transformed, lambda_param = stats.boxcox(data)\n",
    "                    self.df_cleaned[col] = transformed\n",
    "                    print(f\"  ‚Ä¢ {col}: ƒê√£ √°p d·ª•ng Box-Cox (Œª={lambda_param:.3f})\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ {col}: B·ªè qua (c√≥ gi√° tr·ªã <= 0)\")\n",
    "\n",
    "        print(f\"\\n‚úÖ K·∫øt qu·∫£: {len(self.df)} ‚Üí {len(self.df_cleaned)} h√†ng\")\n",
    "        return self.df_cleaned\n",
    "    \n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. X·ª¨ L√ù D·ªÆ LI·ªÜU PH√ÇN LO·∫†I (CATEGORICAL ENCODING)\n",
    "    # ========================================================================\n",
    "\n",
    "    def encode_categorical(self, encoding_map=None, max_categories=10):\n",
    "        \"\"\"\n",
    "        Encode c√°c bi·∫øn ph√¢n lo·∫°i\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        encoding_map : dict\n",
    "            Dictionary ch·ªâ ƒë·ªãnh ph∆∞∆°ng ph√°p encode cho t·ª´ng c·ªôt\n",
    "            V√≠ d·ª•: {'col1': 'onehot', 'col2': 'label', 'col3': 'ordinal'}\n",
    "        max_categories : int\n",
    "            S·ªë categories t·ªëi ƒëa cho one-hot encoding\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üî§ ENCODING D·ªÆ LI·ªÜU PH√ÇN LO·∫†I\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "\n",
    "        if encoding_map is None:\n",
    "            encoding_map = self._auto_encoding_strategy(max_categories)\n",
    "\n",
    "        print(\"\\nüìã Strategies:\")\n",
    "\n",
    "        for col, method in encoding_map.items():\n",
    "            if col not in self.df_cleaned.columns:\n",
    "                continue\n",
    "            \n",
    "            n_unique = self.df_cleaned[col].nunique()\n",
    "            print(f\"\\n  ‚Üí {col} ({n_unique} categories):\")\n",
    "            print(f\"     Method: {method}\")\n",
    "\n",
    "            try:\n",
    "                if method == 'label':\n",
    "                    # Label Encoding\n",
    "                    le = LabelEncoder()\n",
    "                    self.df_cleaned[col] = le.fit_transform(self.df_cleaned[col].astype(str))\n",
    "                    self.encoding_strategies[col] = {'method': 'label', 'encoder': le}\n",
    "                \n",
    "                elif method == 'onehot':\n",
    "                    # One-Hot Encoding\n",
    "                    dummies = pd.get_dummies(self.df_cleaned[col], prefix=col, drop_first=True)\n",
    "                    self.df_cleaned = pd.concat([self.df_cleaned, dummies], axis=1)\n",
    "                    self.df_cleaned.drop(columns=[col], inplace=True)\n",
    "                    self.encoding_strategies[col] = {'method': 'onehot', 'columns': dummies.columns.tolist()}\n",
    "                    print(f\"     ‚úì T·∫°o {len(dummies.columns)} c·ªôt m·ªõi\")\n",
    "\n",
    "                elif method == 'ordinal':\n",
    "                    # Ordinal Encoding (c·∫ßn c√≥ th·ª© t·ª±)\n",
    "                    # User ph·∫£i cung c·∫•p th·ª© t·ª±\n",
    "                    print(f\"     ‚ö†Ô∏è  C·∫ßn cung c·∫•p th·ª© t·ª± cho ordinal encoding\")\n",
    "\n",
    "                elif method == 'frequency':\n",
    "                    # Frequency Encoding\n",
    "                    freq_map = self.df_cleaned[col].value_counts(normalize=True).to_dict()\n",
    "                    self.df_cleaned[col + '_freq'] = self.df_cleaned[col].map(freq_map)\n",
    "                    self.encoding_strategies[col] = {'method': 'frequency', 'map': freq_map}\n",
    "                    print(f\"     ‚úì T·∫°o c·ªôt {col}_freq\")\n",
    "\n",
    "                print(f\"     ‚úì Ho√†n th√†nh\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚úó L·ªói: {str(e)}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Shape sau encoding: {self.df_cleaned.shape}\")\n",
    "        return self.df_cleaned\n",
    "\n",
    "\n",
    "    def _auto_encoding_strategy(self, max_categories):\n",
    "        \"\"\"T·ª± ƒë·ªông ch·ªçn ph∆∞∆°ng ph√°p encoding\"\"\"\n",
    "        strategies = {}\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            \n",
    "            n_unique = self.df[col].nunique()\n",
    "            \n",
    "            if n_unique == 2:\n",
    "                strategies[col] = 'label'  # Binary ‚Üí Label\n",
    "            elif n_unique <= max_categories:\n",
    "                strategies[col] = 'onehot'  # Few categories ‚Üí One-Hot\n",
    "            else:\n",
    "                strategies[col] = 'frequency'  # Many categories ‚Üí Frequency\n",
    "        \n",
    "        return strategies\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # 4. CHU·∫®N H√ìA D·ªÆ LI·ªÜU (FEATURE SCALING)\n",
    "    # ========================================================================\n",
    "\n",
    "    def scale_features(self, method='standard', columns=None):\n",
    "        \"\"\"\n",
    "        Chu·∫©n h√≥a c√°c features s·ªë\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            'standard': StandardScaler (z-score)\n",
    "            'minmax': MinMaxScaler (0-1)\n",
    "            'robust': RobustScaler (s·ª≠ d·ª•ng median, robust v·ªõi outliers)\n",
    "        columns : list\n",
    "            Danh s√°ch c·ªôt c·∫ßn scale (None = t·∫•t c·∫£ numeric)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìè CHU·∫®N H√ìA D·ªÆ LI·ªÜU\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "        \n",
    "        # X√°c ƒë·ªãnh c√°c c·ªôt s·ªë\n",
    "        if columns is None:\n",
    "            columns = [c for c in self.df_cleaned.columns \n",
    "                       if pd.api.types.is_numeric_dtype(self.df_cleaned[c])]\n",
    "            \n",
    "        columns = [c for c in columns if c in self.df_cleaned.columns]\n",
    "\n",
    "        print(f\"\\nMethod: {method}\")\n",
    "        print(f\"S·ªë c·ªôt: {len(columns)}\")\n",
    "\n",
    "        # Ch·ªçn scaler\n",
    "        if method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Method kh√¥ng h·ª£p l·ªá: {method}\")\n",
    "        \n",
    "\n",
    "        # √Åp d·ª•ng scaling\n",
    "        self.df_cleaned[columns] = scaler.fit_transform(self.df_cleaned[columns])\n",
    "        self.scaling_strategies = {'method': method, 'columns': columns, 'scaler': scaler}\n",
    "        \n",
    "\n",
    "        print(f\"\\n‚úÖ ƒê√£ chu·∫©n h√≥a {len(columns)} c·ªôt b·∫±ng {method}\")\n",
    "\n",
    "        # Hi·ªÉn th·ªã th·ªëng k√™ sau scaling\n",
    "        print(\"\\nüìä Th·ªëng k√™ sau scaling (5 c·ªôt ƒë·∫ßu):\")\n",
    "        print(self.df_cleaned[columns[:5]].describe())\n",
    "        \n",
    "        return self.df_cleaned\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # 5. X·ª¨ L√ù D·ªÆ LI·ªÜU TR√ôNG L·∫∂P V√Ä KH√îNG NH·∫§T QU√ÅN\n",
    "    # ========================================================================\n",
    "    \n",
    "    def clean_data_inconsistencies(self):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p v√† kh√¥ng nh·∫•t qu√°n\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üßπ L√ÄM S·∫†CH D·ªÆ LI·ªÜU KH√îNG NH·∫§T QU√ÅN\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if self.df_cleaned is None:\n",
    "            self.df_cleaned = self.df.copy()\n",
    "\n",
    "        # 1. X·ª≠ l√Ω tr√πng l·∫∑p\n",
    "        duplicates_before = self.df_cleaned.duplicated().sum()\n",
    "        print(f\"\\nüîÑ D·ªØ li·ªáu tr√πng l·∫∑p:\")\n",
    "        print(f\"  ‚Ä¢ Tr∆∞·ªõc: {duplicates_before} h√†ng\")\n",
    "\n",
    "        self.df_cleaned.drop_duplicates(inplace=True)\n",
    "\n",
    "        duplicates_after = self.df_cleaned.duplicated().sum()\n",
    "        print(f\"  ‚Ä¢ Sau: {duplicates_after} h√†ng\")\n",
    "        print(f\"  ‚Ä¢ ƒê√£ lo·∫°i b·ªè: {duplicates_before - duplicates_after} h√†ng\")\n",
    "\n",
    "        # 2. Chu·∫©n h√≥a string columns\n",
    "        print(f\"\\nüìù Chu·∫©n h√≥a c·ªôt text:\")\n",
    "        string_cols = [c for c in self.categorical_cols if c in self.df_cleaned.columns]\n",
    "        \n",
    "        for col in string_cols:\n",
    "            if self.df_cleaned[col].dtype == 'object':\n",
    "                # Strip whitespace\n",
    "                self.df_cleaned[col] = self.df_cleaned[col].astype(str).str.strip()\n",
    "\n",
    "                # Lowercase (t√πy ch·ªçn)\n",
    "                # self.df_cleaned[col] = self.df_cleaned[col].str.lower()\n",
    "                \n",
    "                # Thay th·∫ø multiple spaces\n",
    "                self.df_cleaned[col] = self.df_cleaned[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                \n",
    "                print(f\"  ‚úì {col}\")\n",
    "\n",
    "        \n",
    "        print(f\"\\n‚úÖ Shape sau cleaning: {self.df_cleaned.shape}\")\n",
    "        return self.df_cleaned\n",
    "\n",
    "    # ========================================================================\n",
    "    # 6. T·∫†O B√ÅO C√ÅO T·ªîNG K·∫æT\n",
    "    # ========================================================================\n",
    "\n",
    "    def generate_cleaning_report(self):\n",
    "        \"\"\"\n",
    "        T·∫°o b√°o c√°o t·ªïng k·∫øt qu√° tr√¨nh cleaning\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä B√ÅO C√ÅO T·ªîNG K·∫æT QU√Å TR√åNH CLEANING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\nüî¢ Thay ƒë·ªïi v·ªÅ k√≠ch th∆∞·ªõc:\")\n",
    "        print(f\"  ‚Ä¢ H√†ng: {len(self.df):,} ‚Üí {len(self.df_cleaned):,} \"\n",
    "              f\"({(len(self.df_cleaned)-len(self.df))/len(self.df)*100:+.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ C·ªôt: {len(self.df.columns)} ‚Üí {len(self.df_cleaned.columns)} \"\n",
    "              f\"({len(self.df_cleaned.columns)-len(self.df.columns):+d})\")\n",
    "        \n",
    "        print(\"\\nüï≥Ô∏è  Missing values:\")\n",
    "        missing_before = self.df.isnull().sum().sum()\n",
    "        missing_after = self.df_cleaned.isnull().sum().sum()\n",
    "        print(f\"  ‚Ä¢ Tr∆∞·ªõc: {missing_before:,}\")\n",
    "        print(f\"  ‚Ä¢ Sau: {missing_after:,}\")\n",
    "        print(f\"  ‚Ä¢ Gi·∫£m: {missing_before - missing_after:,}\")\n",
    "        \n",
    "        print(\"\\nüîß Strategies ƒë√£ √°p d·ª•ng:\")\n",
    "        print(f\"  ‚Ä¢ Imputation: {len(self.imputation_strategies)} c·ªôt\")\n",
    "        print(f\"  ‚Ä¢ Encoding: {len(self.encoding_strategies)} c·ªôt\")\n",
    "        if self.scaling_strategies:\n",
    "            print(f\"  ‚Ä¢ Scaling: {len(self.scaling_strategies.get('columns', []))} c·ªôt\")\n",
    "        \n",
    "        # L∆∞u cleaned data\n",
    "        print(\"\\nüíæ L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:\")\n",
    "        print(\"  df_cleaned = pipeline.df_cleaned\")\n",
    "        \n",
    "        return self.df_cleaned\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG PART 2\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "C√ÅCH S·ª¨ D·ª§NG:\n",
    "\n",
    "# Kh·ªüi t·∫°o (ti·∫øp t·ª•c t·ª´ Part 1)\n",
    "pipeline = DataCleaningPipeline(df=your_dataframe)\n",
    "\n",
    "# 1. X·ª≠ l√Ω missing values\n",
    "pipeline.handle_missing_values(\n",
    "    strategies={'column1': 'mean', 'column2': 'mode'},\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "# 2. Ph√°t hi·ªán v√† x·ª≠ l√Ω outliers\n",
    "pipeline.detect_outliers(methods=['iqr', 'zscore'], visualize=True)\n",
    "pipeline.handle_outliers(strategy='cap', iqr_multiplier=1.5)\n",
    "\n",
    "# 3. Encode categorical variables\n",
    "pipeline.encode_categorical(max_categories=10)\n",
    "\n",
    "# 4. Scale features\n",
    "pipeline.scale_features(method='standard')\n",
    "\n",
    "# 5. Clean inconsistencies\n",
    "pipeline.clean_data_inconsistencies()\n",
    "\n",
    "# 6. B√°o c√°o t·ªïng k·∫øt\n",
    "cleaned_df = pipeline.generate_cleaning_report()\n",
    "\n",
    "# 7. L∆∞u k·∫øt qu·∫£\n",
    "cleaned_df.to_csv('cleaned_data.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√É HO√ÄN TH√ÄNH PART 2: DATA CLEANING & PREPROCESSING\")\n",
    "print(\"üìå Ti·∫øp theo: Part 3 - Relationship Analysis & Feature Selection\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
